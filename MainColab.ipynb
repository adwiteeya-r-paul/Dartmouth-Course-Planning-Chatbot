{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adwiteeya-r-paul/Dartmouth-Course-Planning-Chatbot/blob/main/MainColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dartmouth Course Planning Chatbot\n",
        "**Adwiteeya R. Paul (arp)** [adwiteeya.r.paul.27@dartmouth.edu]\n",
        "\n",
        "**Description**: This project builds a course planning chatbot that takes user input containing on and off terms, major, distributives etc. in natural language and outputs a table of courses for 4 years.\n",
        "\n",
        "\n",
        "**Single major handling:** I started this project with a single major, Cognitive Science. Cognitive science is an interdisciplinary major and spans across many departments - making it really to plan one's courses keeping course availability, off terms and distributives in mind. It essentially represents a slightly harder form of the usual struggle of Dartmouth students.\n",
        "\n",
        "We used spaCy and sqlite for data parsing.\n",
        "\n",
        "**User input parsing:** Building user input parser is ongoing.\n",
        "\n"
      ],
      "metadata": {
        "id": "fduqWlyHnvk5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9VQzvooFRam"
      },
      "outputs": [],
      "source": [
        "# installing spacy, numpy, pdfminer\n",
        "!pip install -U spacy numpy pdfminer.six transformers\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "from spacy.pipeline import EntityRuler\n",
        "import os\n",
        "\n",
        "# mounting google drive for accessing data\n",
        "mount = '/content/drive'\n",
        "from google.colab import drive\n",
        "drive.mount(mount)\n",
        "\n",
        "# installing pdfminer for pdf handling\n",
        "!pip install pdfminer.six\n",
        "from pdfminer.high_level import extract_text # Import the extraction function\n",
        "\n",
        "#importing sqlite3 for sql database creation\n",
        "import sqlite3\n",
        "import os\n",
        "\n",
        "#importing csv and pandas for csv file creation\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# installing transformers for user input processing\n",
        "!pip install transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to do data extraction from pdf using NER\n",
        "\n",
        "\n",
        "def processdata(file):\n",
        "# Load the English NLP model *without* the 'ner' component\n",
        "  nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
        "\n",
        "# Add the EntityRuler (this will be the only entity recognition component)\n",
        "  ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "  patterns = [\n",
        "   # New pattern for Discipline Code + Number (e.g., \"CLST 6\")\n",
        "    {\"label\": \"COURSE_CODE\", \"pattern\": [{\"TEXT\": \"<\"}, {\"TEXT\": {\"REGEX\": \"[A-Z]{3,4}\"}}, {\"IS_SPACE\": True, \"OP\": \"?\"}, {\"TEXT\": {\"REGEX\": \"\\\\d+\"}}, {\"TEXT\": \">\"}]}, # New pattern for e.g., <COSC 1>\n",
        "\n",
        "\n",
        "     {\"label\": \"PREREQUISITE\", \"pattern\": [\n",
        "        {\"TEXT\": {\"REGEX\": \"^[A-Z]{2,4}$\"}}, # Match 2-4 uppercase letters (Discipline Code)\n",
        "        {\"IS_SPACE\": True, \"OP\": \"?\"},      # Optionally match a space between code and number\n",
        "        {\"TEXT\": {\"REGEX\": \"^\\\\d{1,3}|\\d{1,2}\\.\\d{1,2}$\"}}    # Match 1-3 digits (Course Number)\n",
        "    ]},\n",
        "\n",
        "    # Keep the original single-token pattern just in case (less likely to match standard text)\n",
        "    {\"label\": \"TERM\", \"pattern\": [{\"LOWER\": {\"IN\": [\"fall\", \"winter\", \"spring\", \"summer\"]}}]},\n",
        "    {\"label\": \"DISTRIBUTIVE\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"^(ART|LIT|TMV|INT|SOC|QDS|SCI|SLA|TAS|TLA)$\"}}]},\n",
        "    {\"label\": \"WORLDCULTURE\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"^(W|NW|INT)$\"}}]},\n",
        "\n",
        "]\n",
        "\n",
        "  ruler.add_patterns(patterns)\n",
        "\n",
        "  pdf_file_path = file\n",
        "# Define the output directory for spaCy results\n",
        "  spacy_output_dir = \"spacy_entities_output\"\n",
        "  worksheet_text = \"\" # Initialize to empty\n",
        "\n",
        "# Extract text from the PDF file\n",
        "  try:\n",
        "    worksheet_text = extract_text(pdf_file_path)\n",
        "    print(\"Successfully extracted text from the PDF.\")\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: PDF file not found at {pdf_file_path}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred while extracting text from the PDF: {e}\")\n",
        "\n",
        "# Process the extracted text with spaCy if successfully extracted\n",
        "  if worksheet_text:\n",
        "    print(\"\\nExtracted Text (first 500 chars):\")\n",
        "    print(worksheet_text[:500])\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    doc = nlp(worksheet_text)\n",
        "\n",
        "    # Create the output directory if it doesn't exist\n",
        "    os.makedirs(spacy_output_dir, exist_ok=True)\n",
        "\n",
        "    # Define the output file path (using the PDF name as base)\n",
        "    pdf_name = os.path.basename(pdf_file_path)\n",
        "    output_file_path = os.path.join(spacy_output_dir, f\"{os.path.splitext(pdf_name)[0]}_entities.txt\")\n",
        "\n",
        "    # Prepare the entity data for writing\n",
        "    entity_lines = []\n",
        "    entity_lines.append(\"Detected Entities (from EntityRuler only):\")\n",
        "    if doc.ents:\n",
        "        for ent in doc.ents:\n",
        "          entity_lines.append(f\"{ent.label_}: {ent.text}\")\n",
        "    else:\n",
        "        entity_lines.append(\"No entities detected.\")\n",
        "\n",
        "    # Write the entity data to the output file\n",
        "    try:\n",
        "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(entity_lines))\n",
        "        print(f\"\\n Detected entities saved to '{output_file_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error saving entities to file: {e}\")\n",
        "\n",
        "  else:\n",
        "    print(\"Could not extract text from the PDF.\")\n",
        "\n",
        "  return output_file_path"
      ],
      "metadata": {
        "id": "wcaFM77iHIbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Database to classify data\n",
        "\n",
        "def createdatabase(database, file):\n",
        "\n",
        "  db_path = database\n",
        "  file_path = file\n",
        "\n",
        "# Kill any old version\n",
        "  try:\n",
        "      os.remove(db_path)\n",
        "  except FileNotFoundError:\n",
        "      pass\n",
        "\n",
        "  conn = sqlite3.connect(db_path, timeout=10, isolation_level=None)  # Timeout added\n",
        "  cursor = conn.cursor()\n",
        "  table_name = \"my_table\"\n",
        "\n",
        "  cursor.execute(f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
        "        Course TEXT PRIMARY KEY,\n",
        "        Prerequisite TEXT,\n",
        "        Term TEXT,\n",
        "        Distributive TEXT\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "# Start parsing\n",
        "  course_code = None\n",
        "  prerequisites = []\n",
        "  terms = []\n",
        "  distributives = []\n",
        "\n",
        "  with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        if line.startswith(\"COURSE_CODE:\"):\n",
        "            # Save previous course\n",
        "            if course_code:\n",
        "                cursor.execute(\"BEGIN\")\n",
        "                cursor.execute(f\"\"\"\n",
        "                    INSERT OR IGNORE INTO {table_name} (Course, Prerequisite, Term, Distributive)\n",
        "                    VALUES (?, ?, ?, ?)\n",
        "                \"\"\", (\n",
        "                    course_code,\n",
        "                    ', '.join(prerequisites),\n",
        "                    ', '.join(terms),\n",
        "                    ', '.join(distributives)\n",
        "                ))\n",
        "                cursor.execute(\"COMMIT\")\n",
        "\n",
        "            # Start new course\n",
        "            course_code = line.split(\":\", 1)[1].strip()\n",
        "            prerequisites, terms, distributives = [], [], []\n",
        "\n",
        "        elif line.startswith(\"PREREQUISITE:\"):\n",
        "            prerequisites.append(line.split(\":\", 1)[1].strip())\n",
        "        elif line.startswith(\"TERM:\"):\n",
        "            terms.append(line.split(\":\", 1)[1].strip())\n",
        "        elif line.startswith(\"DISTRIBUTIVE:\"):\n",
        "            distributives.append(line.split(\":\", 1)[1].strip())\n",
        "\n",
        "# Insert last course\n",
        "  if course_code:\n",
        "    cursor.execute(\"BEGIN\")\n",
        "    cursor.execute(f\"\"\"\n",
        "        INSERT OR IGNORE INTO {table_name} (Course, Prerequisite, Term, Distributive)\n",
        "        VALUES (?, ?, ?, ?)\n",
        "    \"\"\", (\n",
        "        course_code,\n",
        "        ', '.join(prerequisites),\n",
        "        ', '.join(terms),\n",
        "        ', '.join(distributives)\n",
        "    ))\n",
        "    cursor.execute(\"COMMIT\")\n",
        "\n",
        "# Show results, testing\n",
        "  cursor.execute(f\"SELECT * FROM {table_name}\")\n",
        "  rows = cursor.fetchall()\n",
        "  for row in rows:\n",
        "    print(row)\n",
        "\n",
        "  conn.close()\n",
        "  return db_path\n"
      ],
      "metadata": {
        "id": "bXRW1mazwL3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting to csv\n",
        "def convert_to_csv(database, output):\n",
        "\n",
        "\n",
        "# Define the database and table names\n",
        "  db_path = database\n",
        "  table_name = \"my_table\"\n",
        "  csv_output_path = output # Name of the output CSV file\n",
        "\n",
        "# Connect to the database\n",
        "  try:\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Select all data from the table\n",
        "    cursor.execute(f\"SELECT * FROM {table_name}\")\n",
        "    rows = cursor.fetchall()\n",
        "\n",
        "    # Get the column names (header)\n",
        "    header = [description[0] for description in cursor.description]\n",
        "\n",
        "    # Write the data to a CSV file\n",
        "    with open(csv_output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "\n",
        "        # Write the header row\n",
        "        csv_writer.writerow(header)\n",
        "\n",
        "        # Write the data rows\n",
        "        csv_writer.writerows(rows)\n",
        "\n",
        "    print(f\"Successfully exported data from '{table_name}' to '{csv_output_path}'\")\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: Database file not found at {db_path}\")\n",
        "  except sqlite3.Error as e:\n",
        "    print(f\"Database error: {e}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "  finally:\n",
        "    # Close the database connection\n",
        "    if 'conn' in locals() and conn:\n",
        "        conn.close()\n",
        "\n",
        "  return csv_output_path"
      ],
      "metadata": {
        "id": "stqntCWvHx2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to access certain category\n",
        "def accesscategory(csv_file, category):\n",
        "  csv_file_path = csv_file\n",
        "\n",
        "  try:\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # --- Select a specific course ---\n",
        "    # Define the course you want to find\n",
        "    target_course = \"<COSC 30>\" # Replace with the actual course code you're looking for\n",
        "\n",
        "    # Filter the DataFrame to find the row(s) where the 'Course' column matches the target course\n",
        "    specific_course_row = df[df['Course'] == target_course]\n",
        "\n",
        "    # Check if the course was found\n",
        "    if not specific_course_row.empty:\n",
        "        print(f\"Details for course '{target_course}':\")\n",
        "        print(specific_course_row)\n",
        "\n",
        "        # You can also access specific information from the found row(s)\n",
        "        # For example, the prerequisite(s):\n",
        "        prerequisites = specific_course_row['Prerequisite'].values\n",
        "        print(f\"\\nPrerequisite(s) for '{target_course}': {prerequisites}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Course '{target_course}' not found in the DataFrame.\")\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: CSV file not found at {csv_file_path}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred while reading the CSV: {e}\")\n"
      ],
      "metadata": {
        "id": "zEQ5pifTMn4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main function: extract text from the pdfs containing department ORC information, prerequisites, focus areas courses\n",
        "# please upload the file change the path of the data #\n",
        "\n",
        "##\n",
        "pre = processdata(\"/prerequisites.pdf\") # change your path here\n",
        "\n",
        "##\n",
        "cs = processdata(\"/cs.pdf\")\n",
        "\n",
        "##\n",
        "cogs = processdata(\"/cogs.pdf\")\n",
        "\n",
        "##\n",
        "psyc = processdata(\"/psyc.pdf\")\n",
        "\n",
        "##\n",
        "ling = processdata(\"/ling.pdf\")\n",
        "\n",
        "##\n",
        "phil = processdata(\"/phil.pdf\")\n",
        "deci = processdata(\"/decision_making.pdf\")\n",
        "lang = processdata(\"/language_and_thought.pdf\")\n",
        "dev = processdata(\"/learning_and_development.pdf\")\n",
        "cons = processdata(\"/consciousness.pdf\")\n",
        "soc = processdata(\"/social_interaction.pdf\")\n",
        "design = processdata(\"/design.pdf\")\n",
        "intel = processdata(\"/intelligence.pdf\")\n",
        "moral= processdata(\"/moral_reasoning.pdf\")\n",
        "\n",
        "predb = createdatabase(\"pre_database.db\", pre)\n",
        "inteldb = createdatabase(\"intel_database.db\", intel)\n",
        "\n",
        "precsv =  convert_to_csv(predb, \"pre.csv\")\n",
        "intelcsv = convert_to_csv(inteldb, \"intel.csv\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XM4CtbXMNNzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pre = pd.read_csv(precsv)\n",
        "df_intel = pd.read_csv(intelcsv)\n",
        "completed = []\n",
        "thesis_pre = df_pre[\n",
        "    df_pre['Course'].str.contains('<COGS 86>', na=False) |\n",
        "    df_pre['Course'].str.contains('<COGS 87>', na=False)\n",
        "]\n",
        "research_pre = df_pre[\n",
        "    df_pre['Course'].str.contains('<COGS 85>', na=False)\n",
        "]\n",
        "seminar_pre = df_pre[\n",
        "    df_pre['Course'].str.contains('<COGS 81>', na=False)\n",
        "]\n",
        "\n",
        "def thesis_filter(df):\n",
        "  plan = df_pre[\n",
        "    ~df_pre['Course'].str.contains('<COGS 86>', na=False) &\n",
        "    ~df_pre['Course'].str.contains('<COGS 87>', na=False) &\n",
        "    ~df_pre['Course'].str.contains('<COGS 85>', na=False) &\n",
        "    ~df_pre['Course'].str.contains('<COGS 81>', na=False) &\n",
        "    ~df_pre['Course'].str.contains('<COGS 80>', na=False)\n",
        "  ]\n",
        "  return plan\n",
        "\n",
        "\n",
        "def season_filter(df, df2, season):\n",
        "  term_condition = (df['Term'].str.contains(season, na=False) | df['Term'].isna()) & (df['Prerequisite'].isna() | df['Prerequisite'].isin(df))\n",
        "  merged_indicator = df_pre.merge(df2, indicator=True, how='left')\n",
        "  rows_unique_to_df_condition = merged_indicator['_merge'] == 'left_only'\n",
        "  combined_condition = term_condition & rows_unique_to_df_condition\n",
        "  plan = df[combined_condition]\n",
        "  return plan\n",
        "\n",
        "\n",
        "\n",
        "df = thesis_filter(df_pre)\n",
        "\n",
        "fall =  df[df['Term'].str.contains('Fall', na=False) & df['Prerequisite'].isna()]\n",
        "\n",
        "fall = fall.sample(n=2)\n",
        "new_row_df = pd.DataFrame([['<WRIT 5>', 'NaN', 'Fall', 'WREQ']], columns=['Course', 'Prerequisite', 'Term', 'Distributive'], index=[fall.shape[0]]) # Replace with data, column names, and desired index\n",
        "fall = pd.concat([fall, new_row_df], axis = 0)\n",
        "fall['Term'] = '25F'\n",
        "\n",
        "\n",
        "winter = season_filter(df, fall, \"Winter\")\n",
        "winter = winter.sample(n=2)\n",
        "new_row_df = pd.DataFrame([['<WRIT 7>', 'WRIT 5', 'Winter', 'WREQ']], columns=['Course', 'Prerequisite', 'Term', 'Distributive'], index=[winter.shape[0]]) # Replace with data, column names, and desired index\n",
        "winter = pd.concat([winter, new_row_df], axis = 0)\n",
        "winter['Term'] = '25W'\n",
        "pd.concat([fall, winter], axis=0)\n",
        "\n",
        "spring = season_filter(df, winter, \"Spring\")\n",
        "\n",
        "spring = spring.sample(n=3)\n",
        "spring['Term'] = '25S'\n",
        "pd.concat([fall, winter, spring], axis=0)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CH2Hq90f8TQA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}